import os
import time
import json
import requests
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException, StaleElementReferenceException

# ğŸ“‚ ì €ì¥ ê²½ë¡œ
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
PDF_DIR = os.path.join(SCRIPT_DIR, "pdf_files")
JSON_FILE = os.path.join(SCRIPT_DIR, "hana_products_with_pdf.json")
os.makedirs(PDF_DIR, exist_ok=True)

# ğŸ“Œ ì´ì „ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸° (ì¬ì‹œì‘)
if os.path.exists(JSON_FILE):
    with open(JSON_FILE, "r", encoding="utf-8") as f:
        results = json.load(f)
else:
    results = []

# ì´ë¯¸ í¬ë¡¤ë§í•œ ìƒí’ˆëª… ë¦¬ìŠ¤íŠ¸
crawled_names = {item.get("ìƒí’ˆëª…") for item in results}

# ğŸ“Œ í¬ë¡¬ ë“œë¼ì´ë²„ ì˜µì…˜
options = webdriver.ChromeOptions()
# options.add_argument("--headless") # í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œ
options.add_argument("--start-maximized")
options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.212 Safari/537.36")
driver = webdriver.Chrome(options=options)
wait = WebDriverWait(driver, 15)

# ğŸ“Œ ë¦¬ìŠ¤íŠ¸ í˜ì´ì§€ ê·¸ë£¹
list_pages = [
    "https://www.kebhana.com/cont/mall/mall09/mall0902/mall090201/index.jsp",
    "https://www.kebhana.com/cont/mall/mall09/mall0902/mall090202/index.jsp",
    "https://www.kebhana.com/cont/mall/mall09/mall0902/mall090203/index.jsp",
    "https://www.kebhana.com/cont/mall/mall09/mall0902/mall090204/index.jsp",
    "https://www.kebhana.com/cont/mall/mall09/mall0903/mall090301/index.jsp",
    "https://www.kebhana.com/cont/mall/mall09/mall0903/mall090302/index.jsp",
    "https://www.kebhana.com/cont/mall/mall09/mall0903/mall090303/index.jsp",
    "https://www.kebhana.com/cont/mall/mall09/mall0903/mall090304/index.jsp",
    "https://www.kebhana.com/cont/mall/mall09/mall0903/mall090305/index.jsp",
    "https://www.kebhana.com/cont/mall/mall09/mall0903/mall090306/index.jsp",
    "https://www.kebhana.com/cont/mall/mall09/mall0903/mall090307/index.jsp"
]

# ğŸ“Œ ê° ì¹´í…Œê³ ë¦¬ í˜ì´ì§€ ìˆœíšŒ
for list_url in list_pages:
    print(f"--- ì¹´í…Œê³ ë¦¬ ì‹œì‘: {list_url} ---")
    driver.get(list_url)
    time.sleep(2)

    page_count = 1
    max_retries_per_page = 3  # ê°™ì€ í˜ì´ì§€ì—ì„œ ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜
    current_retry_count = 0
    last_processed_url = ""
    
    while True:  # í˜ì´ì§€ë„¤ì´ì…˜ ë°˜ë³µ
        current_url = driver.current_url
        print(f"[ë¦¬ìŠ¤íŠ¸ í˜ì´ì§€] í˜„ì¬ URL: {current_url}, í˜ì´ì§€: {page_count}")
        
        # ê°™ì€ URLì—ì„œ ë°˜ë³µë˜ëŠ” ê²½ìš° ì²´í¬
        if current_url == last_processed_url:
            current_retry_count += 1
            print(f"[ê²½ê³ ] ê°™ì€ í˜ì´ì§€ê°€ ë°˜ë³µë©ë‹ˆë‹¤. ì¬ì‹œë„ íšŸìˆ˜: {current_retry_count}/{max_retries_per_page}")
            if current_retry_count >= max_retries_per_page:
                print("[ì—ëŸ¬] ê°™ì€ í˜ì´ì§€ì—ì„œ ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼. ë‹¤ìŒ ì¹´í…Œê³ ë¦¬ë¡œ ì´ë™í•©ë‹ˆë‹¤.")
                break
        else:
            current_retry_count = 0
            last_processed_url = current_url
        
        try:
            # í˜ì´ì§€ ë¡œë“œ ëŒ€ê¸°
            wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, "td:nth-child(1) a")))
            product_links = driver.find_elements(By.CSS_SELECTOR, "td:nth-child(1) a")
            
            successful_crawls = 0  # í˜„ì¬ í˜ì´ì§€ì—ì„œ ì„±ê³µí•œ í¬ë¡¤ë§ ìˆ˜
            error_occurred = False  # í˜„ì¬ í˜ì´ì§€ì—ì„œ ì—ëŸ¬ ë°œìƒ ì—¬ë¶€
            
            for i in range(len(product_links)):
                try:
                    # StaleElementReferenceException ë°©ì§€
                    product_links = wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, "td:nth-child(1) a")))
                    link = product_links[i]
                    product_name = link.text.strip()
                    
                    if not product_name:
                        continue

                    # ì´ë¯¸ ì €ì¥ëœ ìƒí’ˆì´ë©´ ê±´ë„ˆë›°ê¸°
                    if product_name in crawled_names:
                        print(f"[ìŠ¤í‚µ] '{product_name}' (ì´ë¯¸ í¬ë¡¤ë§ ì™„ë£Œ)")
                        continue

                    print(f"  > ìƒí’ˆ í¬ë¡¤ë§ ì‹œì‘: '{product_name}'")
                    
                    # ìƒí’ˆ ë§í¬ í´ë¦­
                    link.click()
                    
                    try:
                        # ìƒí’ˆ ìƒì„¸ í˜ì´ì§€ ë¡œë“œ ëŒ€ê¸°
                        wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, "dl.prodcutInfo")))
                        
                        detail_data = {"ìƒí’ˆëª…": product_name}

                        # ğŸ“Œ dtâ€“dd ìŒ ì¶”ì¶œ
                        try:
                            dts = driver.find_elements(By.CSS_SELECTOR, "dl.prodcutInfo dt")
                            dds = driver.find_elements(By.CSS_SELECTOR, "dl.prodcutInfo dd")
                            if dts and dds:
                                for dt, dd in zip(dts, dds):
                                    dt_text = dt.text.strip()
                                    dd_text = dd.text.strip()
                                    if dt_text and dd_text:
                                        detail_data[dt_text] = dd_text
                        except Exception as e:
                            print(f"[ì—ëŸ¬] '{product_name}' dtâ€“dd ì¶”ì¶œ ì‹¤íŒ¨: {e}")
                            
                        # ğŸ“Œ ê¸ˆë¦¬í‘œ ì¶”ì¶œ
                        rate_table_data = []
                        try:
                            table = driver.find_element(By.CSS_SELECTOR, "table.tbl_type")
                            headers = [th.text.strip() for th in table.find_elements(By.CSS_SELECTOR, "thead th")]
                            rows = table.find_elements(By.CSS_SELECTOR, "tbody tr")
                            if headers and rows:
                                for row in rows:
                                    cells = [td.text.strip().replace('%', '') for td in row.find_elements(By.TAG_NAME, "td")]
                                    if len(headers) == len(cells):
                                        rate_table_data.append({headers[i]: cells[i] for i in range(len(headers))})
                        except (NoSuchElementException, IndexError):
                            pass  # ê¸ˆë¦¬í‘œê°€ ì—†ê±°ë‚˜ í˜•ì‹ì´ ë‹¤ë¥¼ ê²½ìš° ë¬´ì‹œ
                        detail_data["ê¸ˆë¦¬í‘œ"] = rate_table_data
                        
                        # ğŸ“Œ ìƒí’ˆì„¤ëª…ì„œ PDF ë‹¤ìš´ë¡œë“œ
                        pdf_path = ""
                        try:
                            pdf_link_element = driver.find_element(By.LINK_TEXT, "ìƒí’ˆì„¤ëª…ì„œ")
                            link1 = pdf_link_element.get_attribute("href")
                            if link1:
                                pdf_data = requests.get(link1).content
                                pdf_filename = f"{product_name.replace('/', '_').replace(':', '')}.pdf"
                                pdf_path = os.path.join(PDF_DIR, pdf_filename)
                                with open(pdf_path, "wb") as f:
                                    f.write(pdf_data)
                                print(f"  > PDF ì €ì¥ ì™„ë£Œ: {pdf_path}")
                        except NoSuchElementException:
                            print(f"  > '{product_name}' ìƒí’ˆì„¤ëª…ì„œ ë§í¬ ì—†ìŒ")
                        except Exception as e:
                            print(f"  > [ì—ëŸ¬] '{product_name}' PDF ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
                        
                        detail_data["ìƒí’ˆì„¤ëª…ì„œ_ê²½ë¡œ"] = pdf_path

                        # ğŸ“Œ ë°ì´í„° ì €ì¥
                        results.append(detail_data)
                        crawled_names.add(product_name)
                        successful_crawls += 1
                        
                        # ì¦‰ì‹œ JSON ì €ì¥ (ì¤‘ê°„ ì €ì¥)
                        with open(JSON_FILE, "w", encoding="utf-8") as f:
                            json.dump(results, f, ensure_ascii=False, indent=2)

                        print(f"  > ë°ì´í„° ì €ì¥ ì™„ë£Œ: '{product_name}'")
                        
                    except TimeoutException:
                        print(f"[ì—ëŸ¬] '{product_name}' ìƒì„¸ í˜ì´ì§€ ë¡œë“œ ì‹œê°„ ì´ˆê³¼. ë‹¤ìŒ ìƒí’ˆìœ¼ë¡œ ë„˜ì–´ê°‘ë‹ˆë‹¤.")
                        error_occurred = True
                        # íƒ€ì„ì•„ì›ƒ ë°œìƒí•œ ìƒí’ˆì„ í¬ë¡¤ë§ ì™„ë£Œ ëª©ë¡ì— ì¶”ê°€í•˜ì—¬ ì¬ì‹œë„ ë°©ì§€
                        crawled_names.add(product_name)
                    except Exception as e:
                        print(f"[ì—ëŸ¬] '{product_name}' í¬ë¡¤ë§ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {e}")
                        error_occurred = True
                        # ì—ëŸ¬ ë°œìƒí•œ ìƒí’ˆì„ í¬ë¡¤ë§ ì™„ë£Œ ëª©ë¡ì— ì¶”ê°€í•˜ì—¬ ì¬ì‹œë„ ë°©ì§€
                        crawled_names.add(product_name)
                    finally:
                        driver.back()
                        wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, "td:nth-child(1) a")))
                        time.sleep(1) # í˜ì´ì§€ ë¦¬ë¡œë“œ ì•ˆì •í™” ëŒ€ê¸°
                
                except StaleElementReferenceException:
                    print("[ê²½ê³ ] StaleElementReferenceException ë°œìƒ. í˜ì´ì§€ë¥¼ ë‹¤ì‹œ ë¡œë“œí•˜ê³  ì¬ì‹œë„í•©ë‹ˆë‹¤.")
                    break # for ë£¨í”„ë¥¼ ë¹ ì ¸ë‚˜ì™€ while ë£¨í”„ì˜ ì²˜ìŒìœ¼ë¡œ ëŒì•„ê°
                except Exception as e:
                    print(f"[ì—ëŸ¬] ìƒí’ˆ ë¦¬ìŠ¤íŠ¸ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                    continue

        except TimeoutException:
            print("[ì—ëŸ¬] ìƒí’ˆ ë¦¬ìŠ¤íŠ¸ í˜ì´ì§€ ë¡œë“œ ì‹œê°„ ì´ˆê³¼. ë‹¤ìŒ ì¹´í…Œê³ ë¦¬ë¡œ ë„˜ì–´ê°‘ë‹ˆë‹¤.")
            break
        
        # ğŸ“Œ ë‹¤ìŒ í˜ì´ì§€ë¡œ ì´ë™
        try:
            # í˜„ì¬ í˜ì´ì§€ ë²ˆí˜¸ë¥¼ ì •í™•íˆ í™•ì¸
            actual_current_page = page_count  # ê¸°ë³¸ê°’
            
            try:
                # ì‹¤ì œ ì›¹í˜ì´ì§€ì—ì„œ í˜„ì¬ í™œì„±í™”ëœ í˜ì´ì§€ ë²ˆí˜¸ ê°€ì ¸ì˜¤ê¸°
                current_page_element = driver.find_element(By.CSS_SELECTOR, "a.on")
                actual_current_page = int(current_page_element.text.strip())
                print(f"  - ì›¹í˜ì´ì§€ ì‹¤ì œ í˜„ì¬ í˜ì´ì§€: {actual_current_page}")
                
                # ì¶”ì  ì¤‘ì¸ í˜ì´ì§€ì™€ ì‹¤ì œ í˜ì´ì§€ê°€ ë‹¤ë¥´ë©´ ë™ê¸°í™”
                if actual_current_page != page_count:
                    print(f"  - í˜ì´ì§€ ë²ˆí˜¸ ë™ê¸°í™”: {page_count} â†’ {actual_current_page}")
                    page_count = actual_current_page
                    
            except (NoSuchElementException, ValueError):
                print(f"  - í˜„ì¬ í˜ì´ì§€ ë²ˆí˜¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ. ì¶”ì  ì¤‘ì¸ í˜ì´ì§€ ì‚¬ìš©: {page_count}")

            # ì—ëŸ¬ê°€ ë°œìƒí–ˆê±°ë‚˜ ëª¨ë“  ìƒí’ˆì´ ì´ë¯¸ í¬ë¡¤ë§ë˜ì—ˆìœ¼ë©´ ê°•ì œë¡œ ë‹¤ìŒ í˜ì´ì§€ë¡œ
            if error_occurred or successful_crawls == 0:
                print(f"  - ì—ëŸ¬ ë°œìƒ ë˜ëŠ” ìƒˆë¡œìš´ í¬ë¡¤ë§ ì—†ìŒ. ë‹¤ìŒ í˜ì´ì§€ë¡œ ê°•ì œ ì´ë™")
                next_page = actual_current_page + 1
            else:
                next_page = actual_current_page + 1

            # ë‹¤ìŒ í˜ì´ì§€ ë²„íŠ¼ ì°¾ê¸° ë° í´ë¦­
            next_page_xpath = f"//div[@class='paging']//a[normalize-space(text())='{next_page}' and not(@class='on')]"
            
            try:
                next_page_btn = wait.until(EC.element_to_be_clickable((By.XPATH, next_page_xpath)))
                print(f"  - ë‹¤ìŒ í˜ì´ì§€({next_page})ë¡œ ì´ë™ ì‹œë„")
                
                # í˜ì´ì§€ ì´ë™ ì „ í˜„ì¬ URL ì €ì¥
                before_url = driver.current_url
                next_page_btn.click()
                time.sleep(3)  # í˜ì´ì§€ ë¡œë“œ ëŒ€ê¸° ì‹œê°„ ì¦ê°€
                
                # í˜ì´ì§€ ì´ë™ í™•ì¸
                after_url = driver.current_url
                if before_url != after_url:
                    print(f"  - í˜ì´ì§€ ì´ë™ ì„±ê³µ: {before_url} â†’ {after_url}")
                    page_count = next_page
                    current_retry_count = 0  # ì„±ê³µì ìœ¼ë¡œ ì´ë™í–ˆìœ¼ë¯€ë¡œ ì¬ì‹œë„ ì¹´ìš´íŠ¸ ë¦¬ì…‹
                else:
                    print(f"  - í˜ì´ì§€ ì´ë™ ì‹¤íŒ¨: URLì´ ë³€ê²½ë˜ì§€ ì•ŠìŒ")
                    raise Exception("í˜ì´ì§€ ì´ë™ ì‹¤íŒ¨")
                    
            except (NoSuchElementException, TimeoutException):
                print("  - ë‹¤ìŒ í˜ì´ì§€ ë²„íŠ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ. í˜ì´ì§€ë„¤ì´ì…˜ ë§ˆì§€ë§‰ìœ¼ë¡œ íŒë‹¨ë©ë‹ˆë‹¤.")
                break
            except Exception as e:
                print(f"  - [ì—ëŸ¬] ë‹¤ìŒ í˜ì´ì§€ ì´ë™ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                break
                
        except Exception as e:
            print(f"  - [ì—ëŸ¬] í˜ì´ì§€ë„¤ì´ì…˜ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            break
        

print(f"ì´ {len(results)}ê°œ ìƒí’ˆ í¬ë¡¤ë§ ì™„ë£Œ")
driver.quit()